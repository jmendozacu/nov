# # Robots.txt for Magento Community and Enterprise
 
# # GENERAL SETTINGS
 
# Enables robots.txt rules for all crawlers
 
User-agent: *
 
# # Crawl-delay parameter: the number of seconds you want to wait between successful requests to the same server.
# # Set a crawl rate, if your server's traffic problems. Please note that Google ignore crawl-delay setting in Robots.txt. You can set up this in Google Webmaster tool
# Crawl-delay: 30
 
# # Magento sitemap: URL to your sitemap file in Magento
Sitemap: https://www.nov-usa.com/sitemap.xml
 
# Directories
Disallow: /404/
Disallow: /app/
Disallow: /cgi-bin/
Disallow: /dev/
Disallow: /downloader/
Disallow: /errors/
Disallow: /includes/
# Disallow: /js/
# Disallow: /lib/
Disallow: /magento/
# Disallow: /media/
Disallow: /pkginfo/
Disallow: /report/
Disallow: /scripts/
Disallow: /shell/
# Disallow: /skin/
Disallow: /stats/
Disallow: /var/


# Paths (clean URLs)
Disallow: /index.php/
Disallow: /catalog/product_compare/
Disallow: /catalog/category/view/
Disallow: /catalog/product/view/
Disallow: /catalogsearch/
#Disallow: /checkout/
Disallow: /control/
Disallow: /contacts/
Disallow: /customer/
Disallow: /customize/
Disallow: /newsletter/
Disallow: /poll/
Disallow: /review/
Disallow: /sendfriend/
Disallow: /tag/
Disallow: /wishlist/
Disallow: /catalog/product/gallery/

# Site Special Paths

# Files
Disallow: /cron.php
Disallow: /cron.sh
Disallow: /error_log
Disallow: /install.php
Disallow: /LICENSE.html
Disallow: /LICENSE.txt
Disallow: /LICENSE_AFL.txt
Disallow: /STATUS.txt
Disallow: /*.sh

# Paths (no clean URLs)
# Disallow: /*.js$
Disallow: /*.css$
Disallow: /*.php$
Disallow: /*?SID=
Disallow: /*?p=*&
Disallow: /*?*&p=

